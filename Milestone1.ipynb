{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a Spacy Language model\n",
    "sp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'text', 'url'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/data.json', 'r') as outfile:\n",
    "    summaries = json.load(outfile)\n",
    "print(summaries[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> since SCONJ mark\n",
      "<class 'spacy.tokens.token.Token'> i PRON nsubj\n",
      "<class 'spacy.tokens.token.Token'> upgrade VERB advcl\n",
      "<class 'spacy.tokens.token.Token'> my DET poss\n",
      "<class 'spacy.tokens.token.Token'> laptop NOUN dobj\n",
      "<class 'spacy.tokens.token.Token'> is AUX auxpass\n",
      "<class 'spacy.tokens.token.Token'> started VERB ROOT\n",
      "<class 'spacy.tokens.token.Token'> behaving VERB xcomp\n",
      "<class 'spacy.tokens.token.Token'> abnormally ADV advmod\n"
     ]
    }
   ],
   "source": [
    "# Lowercase data. Lowercase the text. \n",
    "# Explore the attributes of each token returned SpaCy.\n",
    "text = \"since I upgrade my laptop is started behaving abnormally\"\n",
    "text_tokenized = sp(text.lower())\n",
    "for token in text_tokenized[:10]:\n",
    "    print(type(token), token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> i PRON nsubj\n",
      "<class 'spacy.tokens.token.Token'> want VERB ROOT\n",
      "<class 'spacy.tokens.token.Token'> to PART aux\n",
      "<class 'spacy.tokens.token.Token'> add VERB xcomp\n",
      "<class 'spacy.tokens.token.Token'> a DET det\n",
      "<class 'spacy.tokens.token.Token'> new ADJ amod\n",
      "<class 'spacy.tokens.token.Token'> column NOUN dobj\n",
      "<class 'spacy.tokens.token.Token'> to ADP prep\n",
      "<class 'spacy.tokens.token.Token'> the DET det\n",
      "<class 'spacy.tokens.token.Token'> forecast NOUN compound\n"
     ]
    }
   ],
   "source": [
    "# Lowercase data. Lowercase the text. \n",
    "# Explore the attributes of each token returned SpaCy.\n",
    "text = \"I want to add a new column to the forecast report\"\n",
    "text_tokenized = sp(text.lower())\n",
    "for token in text_tokenized[:10]:\n",
    "    print(type(token), token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HIV/AIDS, or Human Immunodeficiency Virus, is considered by some authors a global pandemic. However, the WHO currently uses the term \\'global epidemic\\' to describe HIV. As of 2018, approximately 37.9 million people are infected with HIV globally.There were about 770,000 deaths from AIDS in 2018.The 2015 Global Burden of Disease Study, in a report published in The Lancet, estimated that the global incidence of HIV infection peaked in 1997 at 3.3 million per year. Global incidence fell rapidly from 1997 to 2005, to about 2.6 million per year, but remained stable from 2005 to 2015.Sub-Saharan Africa is the region most affected. In 2018, an estimated 61% of new HIV infections occurred in this region. Prevalence ratios are \"In western and central Europe and North America, low and declining incidence of HIV and mortality among people infected with HIV over the last 17 years has seen the incidence:prevalence ratio fall from 0.06 in 2000 to 0.03 in 2017. Strong and steady reductions in new HIV infections and mortality among people infected with HIV in eastern and southern Africa has pushed the ratio down from 0.11 in 2000 to 0.04 in 2017. Progress has been more gradual in Asia and the Pacific (0.05 in 2017), Latin America (0.06 in 2017), the Caribbean (0.05 in 2017) and western and central Africa (0.06 in 2017). The incidence:prevalence ratios of the Middle East and North Africa (0.08 in 2017) and eastern Europe and central Asia (0.09 in 2017)\". South Africa has the largest population of people with HIV of any country in the world, at 7.06 million  as of 2017. In Tanzania, HIV/AIDS was reported to have a prevalence of 4.5% among Tanzanian adults aged 15â€“49 in 2017.South & South-East Asia (a region with about 2 billion people as of 2010, over 30% of the global population) has an estimated 4 million cases (12% of all people infected with HIV), with about 250,000 deaths in 2010. Approximately 2.5 million of these cases are in India, where however the prevalence is only about 0.3% (somewhat higher than that found in Western and Central Europe or Canada). Prevalence is lowest in East Asia at 0.1%.In 2017, approximately 1 million people in the United States had HIV; 14% did not realize that they were infected.In 2017, 93,385 people (64,472 men and 28,877 women) living with diagnosed HIV infection received HIV care in the UK and 428 deaths. 42,739 (nearly 50%) of those are gay or bisexual, a small segment of the overall population. \\nIn Australia, as of 2017, there were about 27,545 cases. \\nIn Canada as of 2016, there were about 63,110 cases.A reconstruction of its genetic history shows that the HIV pandemic almost certainly originated in Kinshasa, the capital of the Democratic Republic of the Congo, around 1920. AIDS was first recognized in 1981, in 1983 the HIV virus was discovered and identified as the cause of AIDS, and by 2009 AIDS caused nearly 30 million deaths.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = summaries[1][\"text\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> hiv PROPN nmod\n",
      "<class 'spacy.tokens.token.Token'> / SYM punct\n",
      "<class 'spacy.tokens.token.Token'> aids PROPN nsubjpass\n",
      "<class 'spacy.tokens.token.Token'> , PUNCT punct\n",
      "<class 'spacy.tokens.token.Token'> or CCONJ cc\n",
      "<class 'spacy.tokens.token.Token'> human ADJ amod\n",
      "<class 'spacy.tokens.token.Token'> immunodeficiency NOUN compound\n",
      "<class 'spacy.tokens.token.Token'> virus NOUN conj\n",
      "<class 'spacy.tokens.token.Token'> , PUNCT punct\n",
      "<class 'spacy.tokens.token.Token'> is AUX auxpass\n"
     ]
    }
   ],
   "source": [
    "# Lowercase data. Lowercase the text. \n",
    "# Explore the attributes of each token returned SpaCy.\n",
    "text_tokenized = sp(text.lower())\n",
    "for token in text_tokenized[:10]:\n",
    "    print(type(token), token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-18-b05f8a2dc9e6>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if token.dep_ is \"\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' ', ''), ('\\n', ''), ('\\n', '')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model were unable to classify some tokens, let's check what \n",
    "# these tokens look like\n",
    "unclassified_tokens = [(token.lemma_, token.dep_) \n",
    "                       for token \n",
    "                       in text_tokenized \n",
    "                       if token.dep_ is \"\"]\n",
    "\n",
    "# Tokens like these are not useful to search, we will remove them in the next step:\n",
    "unclassified_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[hiv,\n",
       " aids,\n",
       " human,\n",
       " immunodeficiency,\n",
       " virus,\n",
       " considered,\n",
       " authors,\n",
       " global,\n",
       " pandemic,\n",
       " currently]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_sw = [word for word \n",
    "                     in text_tokenized \n",
    "                     if not word.is_stop \n",
    "                     and not word.is_punct\n",
    "                    ]\n",
    "tokens_without_sw[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize (tokenize) the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hiv',\n",
       " 'aids',\n",
       " 'human',\n",
       " 'immunodeficiency',\n",
       " 'virus',\n",
       " 'consider',\n",
       " 'author',\n",
       " 'global',\n",
       " 'pandemic',\n",
       " 'currently']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemmas = [token.lemma_ \n",
    "               for token\n",
    "               in tokens_without_sw\n",
    "               if token.dep_]\n",
    "token_lemmas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(document):\n",
    "    text_lowercased = sp(document.lower())\n",
    "    tokens_without_stopwords = [word \n",
    "                                for word \n",
    "                                in text_lowercased\n",
    "                                if not word.is_stop \n",
    "                                and not word.is_punct]\n",
    "    \n",
    "    token_lemmatized = [token.lemma_ \n",
    "               for token\n",
    "               in tokens_without_stopwords\n",
    "               if token.dep_]\n",
    "    \n",
    "    return token_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> i PRON nsubj i\n",
      "<class 'spacy.tokens.token.Token'> want VERB ROOT want\n",
      "<class 'spacy.tokens.token.Token'> to PART aux to\n",
      "<class 'spacy.tokens.token.Token'> add VERB xcomp add\n",
      "<class 'spacy.tokens.token.Token'> a DET det a\n",
      "<class 'spacy.tokens.token.Token'> new ADJ amod new\n",
      "<class 'spacy.tokens.token.Token'> column NOUN dobj column\n",
      "<class 'spacy.tokens.token.Token'> to ADP prep to\n",
      "<class 'spacy.tokens.token.Token'> the DET det the\n",
      "<class 'spacy.tokens.token.Token'> forecast NOUN compound forecast\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['want', 'add', 'new', 'column', 'forecast', 'report']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I want to add a new column to the forecast report\"\n",
    "text_tokenized = sp(text.lower())\n",
    "for token in text_tokenized[:10]:\n",
    "    print(type(token), token.text, token.pos_, token.dep_, token.lemma_)\n",
    "    \n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in summaries:\n",
    "    doc['tokenized_text'] = tokenizer(doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pandemic',\n",
       " 'greek',\n",
       " 'Ï€á¾¶Î½',\n",
       " 'pan',\n",
       " 'Î´á¿†Î¼Î¿Ï‚',\n",
       " 'demos',\n",
       " 'people',\n",
       " 'epidemic',\n",
       " 'infectious',\n",
       " 'disease',\n",
       " 'spread',\n",
       " 'large',\n",
       " 'region',\n",
       " 'instance',\n",
       " 'multiple',\n",
       " 'continent',\n",
       " 'worldwide',\n",
       " 'affect',\n",
       " 'substantial',\n",
       " 'number',\n",
       " 'people',\n",
       " 'widespread',\n",
       " 'endemic',\n",
       " 'disease',\n",
       " 'stable',\n",
       " 'number',\n",
       " 'infected',\n",
       " 'people',\n",
       " 'pandemic',\n",
       " 'widespread',\n",
       " 'endemic',\n",
       " 'disease',\n",
       " 'stable',\n",
       " 'number',\n",
       " 'infected',\n",
       " 'people',\n",
       " 'recurrence',\n",
       " 'seasonal',\n",
       " 'influenza',\n",
       " 'generally',\n",
       " 'exclude',\n",
       " 'occur',\n",
       " 'simultaneously',\n",
       " 'large',\n",
       " 'region',\n",
       " 'globe',\n",
       " 'spread',\n",
       " 'worldwide',\n",
       " 'human',\n",
       " 'history',\n",
       " 'number',\n",
       " 'pandemic',\n",
       " 'disease',\n",
       " 'smallpox',\n",
       " 'tuberculosis',\n",
       " 'fatal',\n",
       " 'pandemic',\n",
       " 'record',\n",
       " 'history',\n",
       " 'black',\n",
       " 'death',\n",
       " 'know',\n",
       " 'plague',\n",
       " 'kill',\n",
       " 'estimate',\n",
       " '75â€“200',\n",
       " 'million',\n",
       " 'people',\n",
       " '14th',\n",
       " 'century',\n",
       " 'term',\n",
       " 'later',\n",
       " 'pandemic',\n",
       " 'include',\n",
       " '1918',\n",
       " 'influenza',\n",
       " 'pandemic',\n",
       " 'spanish',\n",
       " 'flu',\n",
       " 'current',\n",
       " 'pandemic',\n",
       " 'include',\n",
       " 'covid-19',\n",
       " 'sars',\n",
       " 'cov-2',\n",
       " 'hiv',\n",
       " 'aids']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take a look what our tokenized summaries look like:\n",
    "summaries[0]['tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized texts to file:\n",
    "with open('data/summaries.json', 'w') as outfile:\n",
    "    json.dump(summaries, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
